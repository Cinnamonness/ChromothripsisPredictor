#!/usr/bin/env python3
"""
–û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ö—Ä–æ–º–æ—Ç—Ä–∏–ø—Å–∏—Å–∞
"""

import hydra
from omegaconf import DictConfig, OmegaConf
import torch
import numpy as np
import logging
import os
import sys
from typing import Dict
import time
from tqdm import tqdm

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.data.preprocessor import SVDataPreprocessor
from src.data.processing import merge_and_deduplicate_datasets, binarize_labels
from src.data.datasets import create_data_loaders
from src.models.model_factory import ModelFactory
from src.training.trainer import Trainer
from src.visualization.plots import (
    plot_training_history, 
    plot_roc_curves, 
    plot_metrics_comparison,
    plot_confusion_matrices,
    plot_precision_recall_curves
)

logger = logging.getLogger(__name__)

def print_final_results(results: Dict):
    """–ü–µ—á–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∫–æ–Ω—Å–æ–ª—å"""
    print("\n" + "="*60)
    print("–§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø")
    print("="*60)
    
    for model_type, result in results.items():
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –º–µ—Å—Ç–∞
        if 'final_metrics' in result:
            metrics = result['final_metrics']
        elif 'metrics' in result:
            metrics = result['metrics']
        else:
            print(f"\n‚ö†Ô∏è  {model_type.upper()} MODEL: –ú–µ—Ç—Ä–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            continue
        
        print(f"\nüìä {model_type.upper()} MODEL:")
        print(f"   F1 Score:       {metrics.get('f1', 'N/A'):.4f}")
        print(f"   Accuracy:       {metrics.get('accuracy', 'N/A'):.4f}")
        print(f"   Precision:      {metrics.get('precision', 'N/A'):.4f}")
        print(f"   Recall:         {metrics.get('recall', 'N/A'):.4f}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
        if 'auc' in metrics:
            print(f"   AUC:            {metrics['auc']:.4f}")
        if 'specificity' in metrics:
            print(f"   Specificity:    {metrics['specificity']:.4f}")
        if 'balanced_accuracy' in metrics:
            print(f"   Balanced Acc:   {metrics['balanced_accuracy']:.4f}")
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
        f1_score = metrics.get('f1', 0)
        accuracy = metrics.get('accuracy', 0)
        
        if f1_score > 0.9 and accuracy > 0.95:
            print("   ‚úÖ –û–¢–õ–ò–ß–ù–û–ï –ö–ê–ß–ï–°–¢–í–û")
        elif f1_score > 0.7 and accuracy > 0.8:
            print("   üëç –•–û–†–û–®–ï–ï –ö–ê–ß–ï–°–¢–í–û")
        elif f1_score > 0.5:
            print("   ‚ö†Ô∏è  –°–†–ï–î–ù–ï–ï –ö–ê–ß–ï–°–¢–í–û")
        else:
            print("   ‚ùó –ù–ò–ó–ö–û–ï –ö–ê–ß–ï–°–¢–í–û")
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
        if f1_score > 0.95 and accuracy > 0.98:
            print("   ‚ö†Ô∏è  –í–û–ó–ú–û–ñ–ù–û –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï!")
        
        # Confusion Matrix –µ—Å–ª–∏ –µ—Å—Ç—å
        if all(k in metrics for k in ['tp', 'fp', 'fn', 'tn']):
            print(f"   Confusion Matrix: TP={metrics['tp']}, FP={metrics['fp']}, FN={metrics['fn']}, TN={metrics['tn']}")
        
        # –õ—É—á—à–∞—è –ø–æ—Ç–µ—Ä—è –µ—Å–ª–∏ –µ—Å—Ç—å
        if 'best_val_loss' in result:
            print(f"   Best Val Loss:  {result['best_val_loss']:.4f}")

def load_or_process_data(preprocessor, data_format, cfg, force_reprocess=False):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å, 
    –∏–Ω–∞—á–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö
    """
    cache_filename = f"{data_format}_22features_processed_data.pkl"
    
    # –ï—Å–ª–∏ –Ω–µ —Ñ–æ—Ä—Å–∏—Ä—É–µ–º –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫—É –∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç - –∑–∞–≥—Ä—É–∂–∞–µ–º
    if not force_reprocess:
        cached_data = preprocessor.load_processed_data(cache_filename)
        if cached_data is not None:
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω—ã –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å 22 —Ñ–∏—á–∞–º–∏ –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∞ {data_format} ({len(cached_data)} samples)")
            return cached_data
        else:
            logger.info(f"–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å 22 —Ñ–∏—á–∞–º–∏ –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∞ {data_format} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –Ω–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É...")
    
    # –ò–Ω–∞—á–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å 22 —Ñ–∏—á–∞–º–∏
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {data_format} –¥–∞–Ω–Ω—ã—Ö —Å 22 —Ñ–∏—á–∞–º–∏...")
    format_start = time.time()
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º ICGC –¥–∞–Ω–Ω—ã–µ (—Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ) —Å 22 —Ñ–∏—á–∞–º–∏
    icgc_data = preprocessor.process_patient_regions(
        sv_directory=cfg.data.data_paths.sv_directories.icgc,
        output_format=data_format,
        is_training=True
    )
    icgc_time = time.time()
    logger.info(f"ICGC –¥–∞–Ω–Ω—ã–µ —Å 22 —Ñ–∏—á–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∑–∞ {icgc_time - format_start:.2f} —Å–µ–∫ ({len(icgc_data)} samples)")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º TCGA –¥–∞–Ω–Ω—ã–µ (—Ç–µ—Å—Ç–æ–≤—ã–µ) —Å 22 —Ñ–∏—á–∞–º–∏
    tcga_data = preprocessor.process_patient_regions(
        sv_directory=cfg.data.data_paths.sv_directories.tcga,
        output_format=data_format,
        is_training=False
    )
    tcga_time = time.time()
    logger.info(f"TCGA –¥–∞–Ω–Ω—ã–µ —Å 22 —Ñ–∏—á–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∑–∞ {tcga_time - icgc_time:.2f} —Å–µ–∫ ({len(tcga_data)} samples)")
    
    # –ï—Å–ª–∏ –æ–±–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø—É—Å—Ç—ã–µ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
    if len(icgc_data) == 0 and len(tcga_data) == 0:
        logger.warning(f"‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∞ {data_format}!")
        return []
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ –¥–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ–º
    merged_data = merge_and_deduplicate_datasets(
        icgc_data, tcga_data, data_format
    )
    merge_time = time.time()
    logger.info(f"–î–∞–Ω–Ω—ã–µ —Å 22 —Ñ–∏—á–∞–º–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã –∑–∞ {merge_time - tcga_time:.2f} —Å–µ–∫ ({len(merged_data)} samples)")
    
    # –ë–∏–Ω–∞—Ä–∏–∑—É–µ–º –º–µ—Ç–∫–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –∏ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
    if cfg.data.preprocessing.binarize_labels and len(merged_data) > 0:
        merged_data = binarize_labels(
            merged_data, 
            cfg.data.preprocessing.positive_classes
        )
        logger.info(f"–ú–µ—Ç–∫–∏ –±–∏–Ω–∞—Ä–∏–∑–æ–≤–∞–Ω—ã –∑–∞ {time.time() - merge_time:.2f} —Å–µ–∫")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
    if len(merged_data) > 0:
        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å 22 —Ñ–∏—á–∞–º–∏ –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∞ {data_format}...")
        preprocessor.save_processed_data(merged_data, cache_filename)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º scalers –µ—Å–ª–∏ —ç—Ç–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ scaler
        if preprocessor.feature_extractor.scaler is not None:
            preprocessor.save_scalers()
    else:
        logger.warning(f"‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∞ {data_format}")
    
    total_time = time.time() - format_start
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ {data_format} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {total_time:.2f} —Å–µ–∫ ({len(merged_data)} samples)")
    
    return merged_data

@hydra.main(version_base=None, config_path="../configs", config_name="train")
def main(cfg: DictConfig):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    os.makedirs('outputs/models', exist_ok=True)
    os.makedirs('outputs/figures', exist_ok=True)
    os.makedirs('./data/processed_data', exist_ok=True)
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    torch.manual_seed(42)
    np.random.seed(42)
    
    logger.info("–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ö—Ä–æ–º–æ—Ç—Ä–∏–ø—Å–∏—Å–∞")
    logger.info(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:\n{OmegaConf.to_yaml(cfg)}")
    
    try:
        start_time = time.time()
        
        preprocessor = SVDataPreprocessor(cfg)
        logger.info("–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö...")
        processed_data = {}
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º scalers –µ—Å–ª–∏ –µ—Å—Ç—å
        preprocessor.load_scalers()
        
        # –§–ª–∞–≥ –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏ (–º–æ–∂–Ω–æ –≤—ã–Ω–µ—Å—Ç–∏ –≤ –∫–æ–Ω—Ñ–∏–≥)
        force_reprocess = getattr(cfg.data, 'force_reprocess', False)
        
        for data_format in cfg.data.output_formats:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
            merged_data = load_or_process_data(
                preprocessor, data_format, cfg, force_reprocess
            )
            processed_data[data_format] = merged_data
        
        data_processing_time = time.time() - start_time
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {data_processing_time:.2f} —Å–µ–∫")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤...")
        loader_start = time.time()
        loaders = create_data_loaders(
            processed_data.get('sequence', []),
            processed_data.get('cnn', []),
            processed_data.get('graph', []),
            cfg
        )
        logger.info(f"DataLoader'—ã —Å–æ–∑–¥–∞–Ω—ã –∑–∞ {time.time() - loader_start:.2f} —Å–µ–∫")
        
        results = {}
        
        for data_format in cfg.data.output_formats:
            if data_format not in loaders or len(processed_data.get(data_format, [])) == 0:
                logger.warning(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∞ {data_format}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
                continue
                
            logger.info(f"–û–±—É—á–µ–Ω–∏–µ {data_format} –º–æ–¥–µ–ª–∏...")
            
            model_start = time.time()
            model = ModelFactory.create_model(data_format, cfg)
            trainer = Trainer(model, cfg, data_format)
            train_loader, val_loader = loaders[data_format]
            
            logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è {data_format} –º–æ–¥–µ–ª–∏...")
            model_results = trainer.train(train_loader, val_loader)
            trainer.save_model(f"best_{data_format}_model.pth")
            
            results[data_format] = model_results
            logger.info(f"–û–±—É—á–µ–Ω–∏–µ {data_format} –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {time.time() - model_start:.2f} —Å–µ–∫")
        
        # –ü–µ—á–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∫–æ–Ω—Å–æ–ª—å
        print_final_results(results)
        
        # –í —Ñ—É–Ω–∫—Ü–∏–∏ main, –∑–∞–º–µ–Ω–∏—Ç–µ –±–ª–æ–∫ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–∞:
        logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤...")
        plot_start = time.time()

        histories = [results[fmt]['history'] for fmt in results.keys()]
        model_names = list(results.keys())

        plot_training_history(histories, model_names)
        plot_roc_curves(results, model_names)
        plot_metrics_comparison(results)
        plot_confusion_matrices(results)
        plot_precision_recall_curves(results, model_names)

        logger.info(f"–ì—Ä–∞—Ñ–∏–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –∑–∞ {time.time() - plot_start:.2f} —Å–µ–∫")
        total_time = time.time() - start_time
        logger.info(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ! –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.2f} —Å–µ–∫ ({total_time/60:.2f} –º–∏–Ω)")
        
        return results
        
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise

if __name__ == "__main__":
    main()